{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es_iHEzxW190"
   },
   "source": [
    "# TP Coding Convolutional Neural Networks in Pytorch - part 1\n",
    "\n",
    "For any remark or suggestion, please feel free to contact me at:\n",
    "\n",
    "- alasdair.newson@telecom-paris.fr\n",
    "\n",
    "### Objective:\n",
    "\n",
    "We want to implement a Convolutional Neural Network (CNN) for image recognition. For this we will use two well-known datasets, the first simpler and the second more complicated :\n",
    "\n",
    "- MNIST (images of digits)\n",
    "- CIFAR-10 dataset https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "We will first code the simple ConvNet described below using the Pytorch environment : https://pytorch.org/.\n",
    "\n",
    "- In general, the input of a classification CNN is a set of (c,m,n) image tensors, where are the image sizes, and c is the number of channels of the image. For mnist, the number of channels is c=1, and for CIFAR10, it is c=3.\n",
    "- For the first case of mnist images, we apply the following layers: \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we apply zero-padding)\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we apply zero-padding)\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
    "    - A softmax activation function which outputs are the $P(y_c | X)$ (multi-class problem)\n",
    "\n",
    "Here is a visual example of the above architecture, with c=3:\n",
    "\n",
    "<IMG SRC='https://perso.telecom-paristech.fr/anewson/doc/ima_205/images/TP_ima_CNN.png'>\n",
    "        \n",
    "### Your task:\n",
    "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgyu2GBVW192"
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1Qj5KY79W192"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHJ17JDiliHA"
   },
   "source": [
    "### CNN model in Pytorch\n",
    "\n",
    "There are several ways to write a CNN model in pytorch. In this lab, you will be using the _Sequential_ class of pytorch (similarly to Tensorflow). We will see the syntax further on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_S76Wi_W199"
   },
   "source": [
    "## Import data\n",
    "\n",
    "We first import MNIST dataset. We use the ```torch.utils.data.DataLoader``` function of Pytorch to easily iterate over mini-batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BrYw9LK9W19-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 31200628.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 14417483.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 16018537.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# convert input to Pytorch tensors\n",
    "input_transform=transforms.Compose( [transforms.ToTensor()])\n",
    "# extract mnist data\n",
    "mnist_trainset = datasets.MNIST(root='./data',train=True,download=True,transform=input_transform)\n",
    "print(mnist_trainset)\n",
    "\n",
    "#create data loader with smaller dataset size\n",
    "max_mnist_size = 1000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=256, shuffle=True)\n",
    "\n",
    "# download test dataset\n",
    "mnist_testset = datasets.MNIST(root='./data',train=False,download=True,transform=input_transform)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=256, shuffle=True)\n",
    "\n",
    "# extract the actual data and labels\n",
    "X_train = torch.unsqueeze(mnist_trainset.data,axis=1)[0:max_mnist_size]/255.0\n",
    "Y_train = mnist_trainset.targets[0:max_mnist_size]\n",
    "X_test = torch.unsqueeze(mnist_testset.data,axis=1)/255.0\n",
    "Y_test = mnist_testset.targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss6fBjWrAS4U"
   },
   "source": [
    "### Exploring the data\n",
    "\n",
    "The mnist data is downloaded into ```mnist_trainset```. We can explore the dataset manually, although when we will train it, we will use the ```DataLoader``` of Pytorch. ```torch.utils.data.DataLoader``` is a useful function to extract batches of data from a dataset, applying the transformations which we have specified (conversion to Pytorch tensor, normalisation etc).\n",
    "\n",
    "The images are contained in a sub-structure of ```mnist_trainset``` called ```data```. The labels are contained in another sub-structure of ```mnist_trainset``` called ```targets```. Note that these are kept in their native format (the transformations are not applied to them), so to use them we have to apply the transformation manually, as above.\n",
    "\n",
    "__NOTE__ In general, if you want to find out what a structure contains, use the command ```dir()```, this will give you a list of the sub-structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rMqnFhbH9bcq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', '_check_legacy_exist', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_repr_indent', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'root', 'target_transform', 'targets', 'test_data', 'test_file', 'test_labels', 'train', 'train_data', 'train_labels', 'training_file', 'transform', 'transforms']\n",
      "Size of training data :  torch.Size([60000, 28, 28])\n",
      "Size of training labels :  torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(dir(mnist_trainset))\n",
    "\n",
    "print(\"Size of training data : \", mnist_trainset.data.shape)\n",
    "print(\"Size of training labels : \", mnist_trainset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnTkYmu-W1-E"
   },
   "source": [
    "The mnist dataset has 10 classes. These are the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kvJ7McU7W1-F"
   },
   "outputs": [],
   "source": [
    "mnist_list = [ '0', '1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f_7d1NnW1-L"
   },
   "source": [
    "### Display some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9OcnfCwbW1-M"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAHDCAYAAAD2j4/CAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNmElEQVR4nO3de1xUdf748TeoICqM4gUkJa00K0tdUqLMS5JarWl5Sdse6equqWCR2sXKLEvpa1mtlpd2C+3Xmqamlm1Xb10Er/gt17xUpiiCawkoCiic3x99mfbM5wgzw8ycc5jX8/E4f3zefObMG3h38N2Zz+eEaJqmCQAAAADYWKjZCQAAAABATdHYAAAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO3R2AAAAACwPRobi9m+fbukpqbKNddcIw0bNpT4+HgZNmyYHDhwwOzUECRKS0vlsccek7i4OImIiJDExET5/PPPzU4LQWbXrl1y5513SnR0tDRo0EA6duwoc+fONTstBIEzZ87I9OnTpX///hIdHS0hISGyePFis9NCkPj3v/8tQ4cOlcsuu0waNGggzZo1kx49esiHH35odmq2UNfsBKD3P//zP/LNN9/I0KFD5brrrpO8vDx57bXX5A9/+INkZWVJx44dzU4RtdyoUaNk5cqVkpaWJu3atZPFixfL7bffLhs3bpTu3bubnR6CwGeffSYDBgyQLl26yLRp06RRo0by448/ytGjR81ODUHg5MmTMmPGDImPj5dOnTrJpk2bzE4JQeTw4cNy+vRpGTlypMTFxcnZs2dl1apVcuedd8qiRYtk7NixZqdoaSGapmlmJ4HfbdmyRa6//noJCwtzxg4ePCjXXnutDBkyRN555x0Ts0Ntt23bNklMTJQXX3xRpkyZIiIiJSUl0rFjR2nRooVs2bLF5AxR2xUVFUn79u3lxhtvlJUrV0poKB8sQGCVlpbKqVOnJDY2Vnbs2CFdu3aVjIwMGTVqlNmpIUiVl5dLQkKClJSUyL59+8xOx9L4i2ExN954o66pERFp166dXHPNNfL999+blBWCxcqVK6VOnTq6/yNUv359GTNmjGRmZkpOTo6J2SEYLF26VPLz82XmzJkSGhoqxcXFUlFRYXZaCCLh4eESGxtrdhqAU506daR169ZSUFBgdiqWR2NjA5qmSX5+vjRr1szsVFDLZWdnS/v27SUqKkoX79atm4iI7N6924SsEEy++OILiYqKkmPHjsmVV14pjRo1kqioKBk/fryUlJSYnR4ABERxcbGcPHlSfvzxR3nllVfk448/lj59+pidluXR2NjAP//5Tzl27Jjcc889ZqeCWu748ePSsmVLJV4Zy83NDXRKCDIHDx6UCxcuyMCBA6Vfv36yatUqGT16tCxcuFD+/Oc/m50eAATE5MmTpXnz5nLFFVfIlClT5K677pLXXnvN7LQsj80DLG7fvn2SkpIiSUlJMnLkSLPTQS137tw5CQ8PV+L169d3fh3wpzNnzsjZs2dl3Lhxzl3Q7r77bikrK5NFixbJjBkzpF27diZnCQD+lZaWJkOGDJHc3Fx57733pLy8XMrKysxOy/K4Y2NheXl5cscdd4jD4XCufQD8KSIiQkpLS5V45UeAIiIiAp0SgkxljY0YMUIXv/fee0VEJDMzM+A5AUCgdejQQZKTk+X++++XdevWyZkzZ2TAgAHCnl9Vo7GxqMLCQrntttukoKBAPvnkE4mLizM7JQSBli1byvHjx5V4ZYw6hL9V1lhMTIwu3qJFCxEROXXqVMBzAgCzDRkyRLZv385zDatBY2NBJSUlMmDAADlw4ICsW7dOrr76arNTQpDo3LmzHDhwQIqKinTxrVu3Or8O+FNCQoKIiBw7dkwXr1zf1bx584DnBABmq/woeGFhocmZWBuNjcWUl5fLPffcI5mZmbJixQpJSkoyOyUEkSFDhkh5ebm88cYbzlhpaalkZGRIYmKitG7d2sTsEAyGDRsmIiJvvvmmLv6Pf/xD6tatK7169TIhKwAIjBMnTiix8+fPy9tvvy0RERH8z+5qsHmAxUyePFk++OADGTBggPz666/KAznvu+8+kzJDMEhMTJShQ4fK1KlT5cSJE3LFFVfIkiVL5Oeff1b+oQn4Q5cuXWT06NHy1ltvyYULF6Rnz56yadMmWbFihUydOpWPQyIgXnvtNSkoKHDeKfzwww/l6NGjIiIyceJEcTgcZqaHWuyBBx6QoqIi6dGjh1xyySWSl5cn//znP2Xfvn0yZ84cadSokdkpWlqIxiokS+nVq5ds3rz5ol/n1wV/KykpkWnTpsk777wjp06dkuuuu06ee+456devn9mpIUicP39eZs2aJRkZGZKbmyuXXnqppKSkSFpamtmpIUi0adNGDh8+bPi1Q4cOSZs2bQKbEILGsmXL5M0335TvvvtOfvnlF4mMjJSEhASZOHGi3HnnnWanZ3k0NgAAAABsjzU2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD2/NbYvP7669KmTRupX7++JCYmyrZt2/z1VoCC+oOZqD+YjRqEmag/mMUv2z0vX75c7r//flm4cKEkJibKq6++KitWrJD9+/dLixYtqnxtRUWF5ObmSmRkpISEhPg6NdiUpmly+vRpiYuLk9DQqvvxmtSfCDUIFfUHswWqBqk/GOEaCDN5Un+i+UG3bt20lJQU57i8vFyLi4vT0tPTq31tTk6OJiIcHIZHTk6OX+uPGuSo6qD+OMw+/F2D1B9HVQfXQA4zD3fqz+cfRSsrK5OdO3dKcnKyMxYaGirJycmSmZmpzC8tLZWioiLnofG8UFQhMjKyyq97Wn8i1CDcR/3BbL6uQeoPnuAaCDNVV38iflhjc/LkSSkvL5eYmBhdPCYmRvLy8pT56enp4nA4nEd8fLyvU0ItUt1taU/rT4QahPuoP5jN1zVI/cETXANhJnc+mmj6rmhTp06VwsJC55GTk2N2Sggy1CDMRP3BTNQfzEYNwpfq+vqEzZo1kzp16kh+fr4unp+fL7Gxscr88PBwCQ8P93UaCFKe1p8INQjfof5gNv4Gw0xcA2E2n9+xCQsLk4SEBFm/fr0zVlFRIevXr5ekpCRfvx2gQ/3BTNQfzEYNwkzUH0zn1hYVHlq2bJkWHh6uLV68WNu7d682duxYrXHjxlpeXl61ry0sLDR91wUO6x6FhYV+rT9qkKOqg/rjMPvwdw1SfxxVHVwDOcw83Kk/vzQ2mqZp8+bN0+Lj47WwsDCtW7duWlZWlluvo6A5qjrcKeqa1B81yFHVQf1xmH34uwapP46qDq6BHGYe7tSfXx7QWRNFRUXicDjMTgMWVVhYKFFRUX59D2oQF0P9wWz+rkHqD1XhGggzuVN/Pt88AACA6rRp00aJrVy5UoklJCQosTlz5iixKVOm+CQvAIB9mb7dMwAAAADUFI0NAAAAANujsQEAAABge6yxCZC6dfU/6t69eytznnzySSXWo0cPJWa030NGRoZu/MEHHyhztm/frsSOHz+uJgsAPnbNNdfoxh999JEyp3Xr1kps6dKlSmzNmjU+ywsAUHtwxwYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD22DzAD3r16qXEXDcGMNo8wIjRRgFGsVGjRlU5FhH57LPPlNiDDz6oxH744Qe3cgMAd73wwgu6sdFGAStWrFBiqampSuzUqVO+SwwAUGtwxwYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD22Dyghv76178qsblz5yqxevXq6cZFRUXKnEceeUSJffDBB27lcf311+vGzz77rDKnb9++SuyLL75QYv3799eN9+3b51YOsBej2l24cKESmz9/vhKbOHGiX3JC7ZWQkKAbnz17Vpnz/PPPKzE2CoAd9OjRQzfetGmTMmfRokVKbPz48f5KCVV44403lJjR30RXRps3LViwQIkVFBQoMde/rykpKdW+nye6du2qG3fv3l2Zs2XLFiW2detWJfbTTz/pxkY/L6vijg0AAAAA26OxAQAAAGB7NDYAAAAAbI/GBgAAAIDtsXmAB/r166fEjBZbGy0uW79+vW48ZswYZc7Ro0e9zu3jjz/WjXfs2KHMmTlzphIzyuP111/XjW+77TZlTllZmacpIoAcDocSc908Yvbs2coco9odOHCgEvvyyy+rzcFo44vS0tJqXwf7M9qoJDY2VjfOzs5W5uzZs8dvOQH+NHXqVN3Y6Fp61VVXBSodVCM/P1+JnT59Wjdu2LChW+caN26cW/Mef/xxt+b5k+smFxeL5ebm6safffaZMufnn3/2WV6+xB0bAAAAALZHYwMAAADA9mhsAAAAANgea2w8sH37dq9fe9999+nG//nPf2qaTpWMzm/0+c7ExEQl1rNnT934gQceUObMmzevBtnBHaGh6v93aNasmRJ77rnnlNiQIUOUmNG6G3fExcUpsWXLlikx18+UHzt2TJnz1ltvKbGMjAwlduTIEU9ShMX84Q9/UGKu9bFy5cpApQP41MMPP6zEXNeVhYSEKHO++uorv+UEz0ybNk2J7dq1SzeeNWuWMqd9+/Y+y+HQoUNK7OTJkz47v9G/IVwflHwxkZGRunGDBg18klMgcMcGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9tg8wAO//vqrEps+fboSu+yyy5SYvzcLcIdR/r1791ZimZmZuvELL7ygzDFaBLl7927vk4PikUceUWJGD1k1YrRw1eiBca4KCgqU2P/+7/+6df5OnTrpxpdccokyx2jB5r333qvE1q1bpxtPnjxZmQPrMqpdVx999FEAMgF8z+ha6s71dfXq1f5IBz7i+vvZvHmzMsf171xNfP/990osLy/PZ+c3erj2+++/79ZrXR/yvnfvXp/kFAjcsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9Ng+ooeeff16JGS2atiqjDQVcnxh/+eWXK3P69++vxNg8wLcaN26sxBYsWODWa3v06KHErrnmmmpfN3XqVCX297//3a33bNOmjW5stOB//PjxSsyoviZMmFDtnBEjRiixc+fOVZcmAqBJkyZKzJ3F1YAdjB07Vom5bqjyt7/9TZnj+mR7WJvRv482btxoQibucd1c6LHHHnPrdVu2bFFiRptG2QV3bAAAAADYHo0NAAAAANujsQEAAABgex43Nl9++aUMGDBA4uLiJCQkRNasWaP7uqZp8vTTT0vLli0lIiJCkpOT5eDBg77KF0Hum2++of5gGuoPZqMGYSbqD1bn8eYBxcXF0qlTJxk9erTcfffdytdnz54tc+fOlSVLlkjbtm1l2rRp0q9fP9m7d6/Ur1/fJ0lbnevi+9ro1ltvVWKBWGx29uzZoKk/o4X8Row2CkhJSVFirou3y8vLlTmHDx92MzvVzz//rBtPnDhRmWO0eNZoA46WLVvqxgMGDFDmpKenK7G0tLRqsqyZYKo/X3NdXA3vUIOBdddddymxK6+8Uom5Xl9nzZrlt5zMRP0FnuvGPCIi77//vhK79tprdWOja+6ZM2eUmNHfzZ07d7qfoMV43Njcdtttcttttxl+TdM0efXVV+Wpp56SgQMHiojI22+/LTExMbJmzRoZPnx4zbJF0Lv11ltl8ODBhl+j/uBv1B/MRg3CTNQfrM6na2wOHTokeXl5kpyc7Iw5HA5JTEyUzMxMw9eUlpZKUVGR7gC84U39iVCD8A3qD2bjbzDMxDUQVuDTxiYvL09ERGJiYnTxmJgY59dcpaeni8PhcB6tW7f2ZUoIIt7Unwg1CN+g/mA2/gbDTFwDYQWm74o2depUKSwsdB45OTlmp4QgQw3CTNQfzET9wWzUIHzJ4zU2VYmNjRURkfz8fN3i3/z8fOncubPha8LDwyU8PNyXaaCGPvvsM93YaHG6FXlTfyL2r8HKzzL/N6OnvLvGnnzySWWO6+/e1zIyMtx6T9enO1922WXKnPvvv1+JvfTSS0rs6NGjnqTotWCtP3cVFxfrxiUlJSZlUnvxN7hmGjZsqMTuu+8+JebORhgnT570SU52wjXQM5U/r//2zDPPKDGjv3Xu/Lxcr7kiIrfffrsSs/NGAUZ8esembdu2EhsbK+vXr3fGioqKZOvWrZKUlOTLtwIU1B/MRP3BbNQgzET9wQo8vmNz5swZ+eGHH5zjQ4cOye7duyU6Olri4+MlLS1Nnn/+eWnXrp1zq7+4uDgZNGiQL/NGkDpz5oz89NNPzjH1h0Ci/mA2ahBmov5gdR43Njt27JDevXs7x5MmTRIRkZEjR8rixYvl0UcfleLiYhk7dqwUFBRI9+7d5ZNPPmH/cvhEdna2/PGPf3SOqT8EEvUHs1GDMBP1B6sL0Yw+jG+ioqIicTgcZqcR1IYOHaobv/vuu8qczZs3K7E+ffr4LadKhYWFEhUV5df3sHINGj2oKysrS4k1b95cibmuaYiPj1fm/PLLL94n50Pz5s3TjcePH+/W64yek7By5Uqf5CRC/Rn573/kVPrggw+U2O7du3XjP/zhDz7Nw3V9RGRkpDKntLRUiZ06dcqnefibv2vQbvXnS88995wSM3pQstEam7179+rGrg9LrC24BrqnUaNGSsx1Pezjjz+uzLn66qu9fs9//etfurHRXTKjB3PbiTv1Z/quaAAAAABQUzQ2AAAAAGyPxgYAAACA7dHYAAAAALA9nz6gszZp3bq1EjNasGS0WHnZsmVKrGPHjrrx1q1blTk///yzBxn6z4QJE3Rjo4WS7jygDL43duxYJdasWTO3Xrtw4ULd2CobBRi55JJLzE4BbmrSpEnA3zM5OVmJuW440b59e2XOr7/+qsS++OILJeb6oNfa9gA7GDPadMXdv38vvPCCX3KCtRhtajBgwAAlVrlb3H/r1KmTV+9pdI365z//qcRWrFihG9t9owBvcccGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9oJy8wCjjQFSU1N145EjRypzjBZpGy0iNHpSsavCwkIlZrQYbO7cuUrshx9+qPb8NaFpWpVjEZE1a9b4NQeIREdHK7Hx48e79Vqjp6kvWLCgxjn5g+vGGiLqE5qNatDoe/zss898lxh8ytsNR8aNG6fE5syZo8QiIiKqPVfTpk2V2D333KPE+vbtqxsbbURg5c034J2rrrpKiRlde/bt26fEVq9e7ZecEDiRkZG68aBBg5Q5Dz30kBLr0qWLv1ISEZEGDRooscGDB1cbO3/+vDLHaJOLHTt21CA76+GODQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO3V+s0D6tZVv8W33npLifXu3Vs3Nlp05fpkaxGRgoICJXbZZZcpMdfFqFFRUcqcCRMmKLFRo0YpsXfffVeJPfHEE7qxuwtbO3furMSSkpJ046NHjypzFi1a5Nb54b3p06crMaO6MbJ3714l5u9NJ7xltEAzNFT//1wqKiqUOatWrVJiRUVFPssLvmW0CNtVkyZNlJjRRgH169dXYq6br0yZMsWtvF5++WUlNnz4cN14/vz5yhyjTQdgbzfffLMSM6rbI0eOKLGzZ8/6JScEznPPPacbT5w40aRM9G688UafneuOO+5QYs8884wSy8jI0I1PnDjhsxz8jTs2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtlfrNw9IT09XYq4bBYiIfP/997pxamqqMmfz5s0+y8voqbFGmwf06tVLiY0ZM0aJuS74f/bZZ5U5Rout33vvPSVWr1493dhos4XS0lIlBt8yWpzszgJsEZGZM2f6Oh2fGDJkiBJ79NFHlZjrZgFG3/e0adN8lxi8ZrRRhRHX64rrWERkwYIFSiwiIkKJGT3lfezYsbrxuXPn3Mpr9uzZSsx18wCjuoW9Pfnkk0rM6DpjFLPq9RU1c8kll/jsXEYb3vy///f/dOOPP/5YmfPdd995/Z4pKSm6cffu3ZU51113nRKbNWuWErtw4YJubLSJi1VxxwYAAACA7dHYAAAAALA9GhsAAAAAtlfr19hMmjRJiRl9ZnbFihW6sS/X0xgxWu9iFDP6jOTSpUuV2NVXX60buz5cScT4IUyXX365EnP9+SxbtkyZA99zfeig60MqL2b79u1K7NNPP/VJTjVhlP99992nxBo0aKDEysvLdWOjz8O7+xBa+NfOnTvdmud6jbr99tuVOY0bN3brXK4P0hNxf02NN3766Se/nRuB0bBhQ9343nvvVeaEhIQosa+++kqJff31175LDJbhum7T6EGsruuxRUQ+/PBDt86fn5/vXWJucn2gaExMjDInNzfXrXO5rll0fQCyiEheXp4H2QUOd2wAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsr9ZvHlBQUKDEjBaoNm/eXDeOiopS5hQVFfkqLbcZLVK85ZZblNgXX3yhG7du3VqZ47p4V8R4gffTTz+tGx84cKDaPFFzAwYM0I2jo6Pdet2OHTv8kU6NPfLII0rsj3/8o1uv3bhxo2784osv+iQnBIbRw2VdH05ntAmK0fXaaEH38ePHvU/OhdHDkl3f86WXXvLZ+8Ecd911l2585ZVXKnOMNhYyWiyO2mnfvn268eTJk70+V1hYmBJz3cCiuLjY6/O748SJE0qsXbt2SuzgwYNK7IorrtCNX331VWWO64OMrYI7NgAAAABsj8YGAAAAgO3R2AAAAACwPRobAAAAALZX6zcPMHq69Zo1a5TY+PHjdWOjBVbDhg1TYmZsKNCqVSsl5roIzWgRpJGKigolNmHCBN348OHDyhyjJ/Ju2rTJrfeEb+3Zs8fsFEREXWz4xBNPuPU6o/xHjBjhk5xgjhUrVigxd+ojNjZWiRldy1JTU5XYrFmzdOP69esrc+bOnavEjBbA7t+/XzfOyMhQ5sBeXDcPMNqU4uzZs0rsb3/7m99yQu0QGRmpxP71r38pMddNqjp06OC3nESMr50XLlzw6lwJCQlKzPX7ERH5z3/+49X5fYk7NgAAAABsj8YGAAAAgO3R2AAAAACwPY8am/T0dOnatatERkZKixYtZNCgQcpnkUtKSiQlJUWaNm0qjRo1ksGDB0t+fr5Pk0bw6tWrF/UH08yZM4drIEzFNRBm4hoIqwvR3F1lLiL9+/eX4cOHS9euXeXChQvyxBNPyJ49e2Tv3r3OJ6qOHz9ePvroI1m8eLE4HA5JTU2V0NBQ+eabb9x6j6KiInE4HN59N24aOXKkEps9e7Zu3LRpU2WO0VOxX3vtNSXmzo80Li5OiQ0cOFCJGS1wjIqKUmL16tXTjc+fP6/M+frrr5VYly5dlFjjxo2VmCuj83/xxRdKbMCAAdWeyxPz58+XHj16+K3+RAJTg0aGDh2qGy9btsyt1/Xs2VOJGf2uvRUdHa3EXDfbEBGZMWNGtec6evSoEnvwwQeV2Nq1a93MLrD69Okj9913n+2vgVZgtGj/rbfeUmJGmwAYXWMPHDigG0dERChz4uPjldju3buV2B133KEbHz9+XJljFn9fA2tr/ZWXl+vGRjWUnZ2txLp27eq3nOwo2K+BTZo0UWIrV65UYt27d1di1113nW7s2hD6Wp06dZTYU089pcSefvrpas+1bds2JXbrrbcqsTNnzriZnXcKCwsN/w383zzaFe2TTz7RjRcvXiwtWrSQnTt3So8ePaSwsFDefPNNWbp0qdxyyy0i8ttuMldddZVkZWXJDTfcoJyztLRUSktLnWMzdhmDffzpT39yFrUv6k+EGoT73n//fd1FlWsgAs3X10DqD57gGgirq9Eam8LCQhH5/f/o7ty5U86fPy/JycnOOR06dJD4+HjJzMw0PEd6ero4HA7n0bp165qkhCDii/oToQbhPa6BMBP1B7NRg7AarxubiooKSUtLk5tuukk6duwoIiJ5eXkSFhamfJQpJiZG8vLyDM8zdepUKSwsdB45OTnepoQg4qv6E6EG4R2ugTAT9QezUYOwIq8f0JmSkiJ79uyp8Wf5w8PDJTw8vEbn8NSSJUuUmOtDAt944w1lTqdOnZTYtGnTlJgHy5aqZbTG5sSJE0rM9Xv6+OOPlTmbN29WYkafVezbt69ubLQmyej3/umnnyoxf/FV/YmYU4NGXC/m/31rvlJYWJgSu+eee5SYOz+XO++8U4kZrYm6+eablZjRA2xd637v3r3KnLFjxyqxrKysKvO0KjtfA63AaA2Z6zoZEeM6feSRR5RY+/btq31P17WUIiILFixQYlZaU3Mx1J9nQkP1/x/X6OHUX331VaDSqRWCsQbffPNNJdarVy8l9s477ygxf6+pcb3T9Ze//EWZY7TGxsj//u//6sZG62r9vZ7GW17dsUlNTZV169bJxo0bpVWrVs54bGyslJWVKYvs8/PzDZ8oDXiD+oPZqEGYifqD2ahBWJVHjY2maZKamiqrV6+WDRs2SNu2bXVfT0hIkHr16sn69eudsf3798uRI0ckKSnJNxkjqE2ZMoX6g2m4BsJsXANhJq6BsDqPPoqWkpIiS5culbVr10pkZKTz85IOh0MiIiLE4XDImDFjZNKkSRIdHS1RUVEyceJESUpKuuiOVIAn3nvvPeoPppk8ebKsXLmSGoRpuAbCTFwDYXUeNTaVn0V2/TxhRkaGjBo1SkREXnnlFQkNDZXBgwdLaWmp9OvXT+bPn++TZIHCwkLqD6ap/Hw1NQizcA2EmbgGwuo8ekBnIFjlwUyVD5r6b5deeqkSM1pQZaRyx5BKrpsVXIzRwlajPd6NHnxYG7nzcKaaskoNGu0g06xZMyV27tw5JXb27Fkl5vqfutFDaI02qzBitPB269atuvGgQYOUOb/88otb57eqYKo/WJO/a7C21p87D+icMGGCEjPaSCiYBfs10Oghrq4P3hQx/jfZ5MmTfZaH678pRcTZWFaqybbZCxcu1I1TUlK8PpcvuVN/NXqODQAAAABYAY0NAAAAANujsQEAAABgezQ2AAAAAGzPo13RgklxcbESM3qS+sSJEwORDoLQgw8+qMT+9re/KbHmzZsrsQYNGigxb/cJyc3NVWJGTzT+7LPPvDo/APhbaKj+/+MabYDi7uYpQHX++6GllZYvX25CJnpG/7a99957ldiGDRsCkY5fcMcGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9tg8ALCo9957T4lt2bJFif31r39VYldccYUSu+eee3TjjIwMZc5PP/2kxIzm5eXlKTEAsKr+/fvrxkuWLFHm/Pvf/w5UOrCp8ePHK7FPPvlEiUVGRgYiHZ0dO3boxkZ5vfzyy0qssLDQbzmZgTs2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtheiefs4cj8pKioSh8NhdhqwqMLCQomKivLre1CDuBjqD2bzdw1Sf6gK10CYyZ36444NAAAAANujsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO1ZrrHRNM3sFGBhgagPahAXQ/3BbP6uD+oPVeEaCDO5UxuWa2xOnz5tdgqwsEDUBzWIi6H+YDZ/1wf1h6pwDYSZ3KmNEM1irXFFRYXk5uZKZGSknD59Wlq3bi05OTkSFRVldmoeKSoqsm3uItbLX9M0OX36tMTFxUloqH/78coa1DRN4uPjLfMz8JTVfoeeslL+1J/nrPT784bV8g9UDfI32Bqslj/XQM9Z7XfoKSvl70n91Q1QTm4LDQ2VVq1aiYhISEiIiIhERUWZ/kP1lp1zF7FW/g6HIyDvU1mDRUVFImKtn4E3yN83qD/vkL/vBKIG+RtsLVbKn2ugd8jfN9ytP8t9FA0AAAAAPEVjAwAAAMD2LN3YhIeHy/Tp0yU8PNzsVDxm59xF7J+/L9j9Z0D+9mb375/87c/OPwM75y5i//x9we4/A/I3h+U2DwAAAAAAT1n6jg0AAAAAuIPGBgAAAIDt0dgAAAAAsD0aGwAAAAC2R2MDAAAAwPYs29i8/vrr0qZNG6lfv74kJibKtm3bzE7J0JdffikDBgyQuLg4CQkJkTVr1ui+rmmaPP3009KyZUuJiIiQ5ORkOXjwoDnJukhPT5euXbtKZGSktGjRQgYNGiT79+/XzSkpKZGUlBRp2rSpNGrUSAYPHiz5+fkmZRxY1KD/UYMXR/35H/V3cdSf/1F/VaMG/a821qAlG5vly5fLpEmTZPr06bJr1y7p1KmT9OvXT06cOGF2aori4mLp1KmTvP7664Zfnz17tsydO1cWLlwoW7dulYYNG0q/fv2kpKQkwJmqNm/eLCkpKZKVlSWff/65nD9/Xvr27SvFxcXOOQ8//LB8+OGHsmLFCtm8ebPk5ubK3XffbWLWgUENBgY1aIz6Cwzqzxj1FxjU38VRg4FRK2tQs6Bu3bppKSkpznF5ebkWFxenpaenm5hV9UREW716tXNcUVGhxcbGai+++KIzVlBQoIWHh2vvvvuuCRlW7cSJE5qIaJs3b9Y07bdc69Wrp61YscI55/vvv9dERMvMzDQrzYCgBs1BDf6G+jMH9fcb6s8c1N/vqEFz1IYatNwdm7KyMtm5c6ckJyc7Y6GhoZKcnCyZmZkmZua5Q4cOSV5enu57cTgckpiYaMnvpbCwUEREoqOjRURk586dcv78eV3+HTp0kPj4eEvm7yvUoHmoQerPTNQf9Wcm6u831KB5akMNWq6xOXnypJSXl0tMTIwuHhMTI3l5eSZl5Z3KfO3wvVRUVEhaWprcdNNN0rFjRxH5Lf+wsDBp3Lixbq4V8/clatAc1OBvqD9zUH+/of7MQf39jho0R22pwbpmJwBrSElJkT179sjXX39tdioIUtQgzET9wUzUH8xWW2rQcndsmjVrJnXq1FF2XMjPz5fY2FiTsvJOZb5W/15SU1Nl3bp1snHjRmnVqpUzHhsbK2VlZVJQUKCbb7X8fY0aDDxq8HfUX+BRf7+j/gKP+tOjBgOvNtWg5RqbsLAwSUhIkPXr1ztjFRUVsn79eklKSjIxM8+1bdtWYmNjdd9LUVGRbN261RLfi6ZpkpqaKqtXr5YNGzZI27ZtdV9PSEiQevXq6fLfv3+/HDlyxBL5+ws1GDjUoIr6CxzqT0X9BQ71Z4waDJxaWYOmbl1wEcuWLdPCw8O1xYsXa3v37tXGjh2rNW7cWMvLyzM7NcXp06e17OxsLTs7WxMR7eWXX9ays7O1w4cPa5qmaS+88ILWuHFjbe3atdq3336rDRw4UGvbtq127tw5kzPXtPHjx2sOh0PbtGmTdvz4cedx9uxZ55xx48Zp8fHx2oYNG7QdO3ZoSUlJWlJSkolZBwY1GBjUoDHqLzCoP2PUX2BQfxdHDQZGbaxBSzY2mqZp8+bN0+Lj47WwsDCtW7duWlZWltkpGdq4caMmIsoxcuRITdN+2+pv2rRpWkxMjBYeHq716dNH279/v7lJ/x+jvEVEy8jIcM45d+6cNmHCBK1JkyZagwYNtLvuuks7fvy4eUkHEDXof9TgxVF//kf9XRz153/UX9WoQf+rjTUYomma5pt7PwAAAABgDsutsQEAAAAAT9HYAAAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9mhsAAAAANgejQ0AAAAA26OxAQAAAGB7NDYAAAAAbI/GBgAAAIDt0dgAAAAAsD0aGwAAAAC2R2MDAAAAwPZobAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9mhsAAAAANgejQ0AAAAA26OxAQAAAGB7NDYAAAAAbI/GBgAAAIDt0dgAAAAAsD0aGwAAAAC2R2MDAAAAwPZobAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO3R2AAAAACwPRobAAAAALZHYwMAAADA9mhsAAAAANgejQ0AAAAA26OxAQAAAGB7NDYAAAAAbI/GBgAAAIDt0dgAAAAAsD0aGwAAAAC2R2MDAAAAwPZobAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjY0GlpaXy2GOPSVxcnEREREhiYqJ8/vnnZqeFIHHmzBmZPn269O/fX6KjoyUkJEQWL15sdloIMrt27ZI777xToqOjpUGDBtKxY0eZO3eu2WkhCBw8eFCGDx8urVq1kgYNGkiHDh1kxowZcvbsWbNTQxDYuXOn9O/fX6KioiQyMlL69u0ru3fvNjst26hrdgJQjRo1SlauXClpaWnSrl07Wbx4sdx+++2yceNG6d69u9npoZY7efKkzJgxQ+Lj46VTp06yadMms1NCkPnss89kwIAB0qVLF5k2bZo0atRIfvzxRzl69KjZqaGWy8nJkW7duonD4ZDU1FSJjo6WzMxMmT59uuzcuVPWrl1rdoqoxXbt2iXdu3eX1q1by/Tp06WiokLmz58vPXv2lG3btsmVV15pdoqWF6JpmmZ2Evjdtm3bJDExUV588UWZMmWKiIiUlJRIx44dpUWLFrJlyxaTM0RtV1paKqdOnZLY2FjZsWOHdO3aVTIyMmTUqFFmp4YgUFRUJO3bt5cbb7xRVq5cKaGhfLAAgTNr1ix58sknZc+ePXLNNdc44yNHjpS3335bfv31V2nSpImJGaI2u+OOOyQzM1MOHjwoTZs2FRGR48ePS/v27aVv376yatUqkzO0Pv5iWMzKlSulTp06MnbsWGesfv36MmbMGMnMzJScnBwTs0MwCA8Pl9jYWLPTQJBaunSp5Ofny8yZMyU0NFSKi4uloqLC7LQQJIqKikREJCYmRhdv2bKlhIaGSlhYmBlpIUh89dVXkpyc7GxqRH6rvZ49e8q6devkzJkzJmZnDzQ2FpOdnS3t27eXqKgoXbxbt24iInzOEkCt9sUXX0hUVJQcO3ZMrrzySmnUqJFERUXJ+PHjpaSkxOz0UMv16tVLRETGjBkju3fvlpycHFm+fLksWLBAHnzwQWnYsKG5CaJWKy0tlYiICCXeoEEDKSsrkz179piQlb3Q2FjM8ePHpWXLlkq8MpabmxvolAAgYA4ePCgXLlyQgQMHSr9+/WTVqlUyevRoWbhwofz5z382Oz3Ucv3795fnnntOPv/8c+nSpYvEx8fL8OHDZeLEifLKK6+YnR5quSuvvFKysrKkvLzcGSsrK5OtW7eKiMixY8fMSs022DzAYs6dOyfh4eFKvH79+s6vA0BtdebMGTl79qyMGzfOuQva3XffLWVlZbJo0SKZMWOGtGvXzuQsUZu1adNGevToIYMHD5amTZvKRx99JLNmzZLY2FhJTU01Oz3UYhMmTJDx48fLmDFj5NFHH5WKigp5/vnn5fjx4yLCvwHdQWNjMREREVJaWqrEKz+CYXSLEgBqi8pr3IgRI3Txe++9VxYtWiSZmZk0NvCbZcuWydixY+XAgQPSqlUrEfmtsa6oqJDHHntMRowYoVv/APjSuHHjJCcnR1588UVZsmSJiIhcf/318uijj8rMmTOlUaNGJmdofXwUzWJatmzp7Mz/W2UsLi4u0CkBQMBUXuNcF2+3aNFCREROnToV8JwQPObPny9dunRxNjWV7rzzTjl79qxkZ2eblBmCxcyZMyU/P1+++uor+fbbb2X79u3ODVTat29vcnbWR2NjMZ07d5YDBw44d2apVPn5ys6dO5uQFQAERkJCgoionyWvXF/YvHnzgOeE4JGfn69b31Dp/PnzIiJy4cKFQKeEINSkSRPp3r27XHvttSLy26YqrVq1kg4dOpicmfXR2FjMkCFDpLy8XN544w1nrLS0VDIyMiQxMVFat25tYnYA4F/Dhg0TEZE333xTF//HP/4hdevWde5aBfhD+/btJTs7Ww4cOKCLv/vuuxIaGirXXXedSZkhWC1fvly2b98uaWlpPNfLDayxsZjExEQZOnSoTJ06VU6cOCFXXHGFLFmyRH7++WflDz3gL6+99poUFBQ4/y/5hx9+6Hzq+8SJE8XhcJiZHmqxLl26yOjRo+Wtt96SCxcuSM+ePWXTpk2yYsUKmTp1Kh/HhV898sgj8vHHH8vNN98sqamp0rRpU1m3bp18/PHH8pe//IX6g199+eWXMmPGDOnbt680bdpUsrKyJCMjQ/r37y8PPfSQ2enZQoimaZrZSUCvpKREpk2bJu+8846cOnVKrrvuOnnuueekX79+ZqeGINGmTRs5fPiw4dcOHTokbdq0CWxCCCrnz5+XWbNmSUZGhuTm5sqll14qKSkpkpaWZnZqCALbtm2TZ555RrKzs+WXX36Rtm3bysiRI+XRRx+VunX5/8Hwnx9//FEmTJggu3btktOnTztrb9KkSTwc1k00NgAAAABsjw/rAQAAALA9GhsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADb81tj8/rrr0ubNm2kfv36kpiYKNu2bfPXWwEK6g9mov5gNmoQZqL+YBa/bPe8fPlyuf/++2XhwoWSmJgor776qqxYsUL2798vLVq0qPK1FRUVkpubK5GRkRISEuLr1GBTmqbJ6dOnJS4urton79ak/kSoQaioP5gtUDVI/cEI10CYyZP6E80PunXrpqWkpDjH5eXlWlxcnJaenl7ta3NycjQR4eAwPHJycvxaf9QgR1UH9cdh9uHvGqT+OKo6uAZymHm4U38+/yhaWVmZ7Ny5U5KTk52x0NBQSU5OlszMTGV+aWmpFBUVOQ+N54WiCpGRkVV+3dP6E6EG4T7qD2bzdQ1Sf/AE10CYqbr6E/HDGpuTJ09KeXm5xMTE6OIxMTGSl5enzE9PTxeHw+E84uPjfZ0SapHqbkt7Wn8i1CDcR/3BbL6uQeoPnuAaCDO589FE03dFmzp1qhQWFjqPnJwcs1NCkKEGYSbqD2ai/mA2ahC+VNfXJ2zWrJnUqVNH8vPzdfH8/HyJjY1V5oeHh0t4eLiv00CQ8rT+RKhB+A71B7PxNxhm4hoIs/n8jk1YWJgkJCTI+vXrnbGKigpZv369JCUl+frtAB3qD2ai/mA2ahBmov5gOre2qPDQsmXLtPDwcG3x4sXa3r17tbFjx2qNGzfW8vLyqn1tYWGh6bsucFj3KCws9Gv9UYMcVR3UH4fZh79rkPrjqOrgGshh5uFO/fmlsdE0TZs3b54WHx+vhYWFad26ddOysrLceh0FzVHV4U5R16T+qEGOqg7qj8Psw981SP1xVHVwDeQw83Cn/vzygM6aKCoqEofDYXYasKjCwkKJiory63tQg7gY6g9m83cNUn+oCtdAmMmd+vP55gEAAPjKDTfcoMSMtoN96KGHdOPWrVsrc4YNG6bEsrKyapAdAMBKTN/uGQAAAABqisYGAAAAgO3R2AAAAACwPdbYWEhN9nHo3bu3brxp06YaZgMAgbV8+XIllpiYqMSM1s/MmTNHN37//feVOayngS8YrdWaOnWqbty5c2dlzhdffKHE7rjjDiVWVlbmfXJAkOOODQAAAADbo7EBAAAAYHs0NgAAAABsj8YGAAAAgO2xeYCFGC3479Wrl1uv3bhxo2787LPPKnOeeeYZL7JCsIuOjlZi69atU2JNmjTRjY1qcNmyZb5LDLbiuuDa9YGaIiJJSUlK7JtvvlFibdq08VleQKXLL79cia1atUqJdejQQYnVq1dPN66oqFDm3HLLLW6d/95779WNT58+rSYLwBB3bAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPzQM8oGmaEjNa8G+0aNponqvevXsrMddNAUTc21Bg+vTp1c4RYUMBVK99+/ZKLCEhQYnVrau/nMydO1eZ88knnyixgoIC75ODJT388MNK7KWXXtKNjRZXGz3RPSsry3eJAVXo1q2bErv22mvdem1JSYluvGHDBmXO7bff7lasR48euvFHH33kVg4AuGMDAAAAoBagsQEAAABgezQ2AAAAAGyPxgYAAACA7bF5wEUYLdo3YrSQ32jzAG8ZbShgtODfnc0CjOYYbWrgzkYHCB6HDx9WYkVFRUosOjpaNz537pwy58KFC75LDJZgtODfdaMAEZHQUP3/R9uyZYsyZ+XKlb5LDPCBwsJCJTZ58mQltnfvXt14+/btypxFixYpsdGjRyuxt956Sze+//77lTmffvqpmiwA7tgAAAAAsD8aGwAAAAC2R2MDAAAAwPZYY3MR7jwEU8ScNSpGa2x69uypG7ubv9FaItd1Pay5CW7nz59XYkYPV3RVr149JRYSEuKTnGAdDz30kBJzpz5effVVP2QD+FZpaakS27lzpxL79ttvqz3XnDlzlNgNN9ygxFwfivz4448rc1hjY39XXHGFEhsxYoQSy8nJUWJGf5dda6l58+bKnOeff16J7dmzp8o87YY7NgAAAABsj8YGAAAAgO3R2AAAAACwPRobAAAAALbH5gH/x2hBviujRfRGD9A0g2semqZ5fS7XB3myeUBwM3pAXVlZWbWv279/vxIzemgn7GP58uVK7MYbb1RiRpsHHDt2rMoxYEUtWrRQYvHx8UrMnc0D9u3bp8SuvfZaJea6gLy4uLjac8P67rzzTt141apVypy6ddV/lruzGYuIe5vz3HLLLUosOTlZiblTz1bFHRsAAAAAtkdjAwAAAMD2aGwAAAAA2B6NDQAAAADbC8rNA3r16qXEXBfMG9m8ebMfsvGPZ599Vom58z2KqD8fo58XGwpY2+WXX67EfvzxR6/O9fe//12JNW3atNrXeft+sC6jTUmMFrYaxYYNG6YbZ2Vl+S4xIICMNsxYt26dz87/7rvv+uxcMMeuXbuqnZOXl6fELrnkEiXmzqYA7mrWrJkSS0hIUGJsHgAAAAAAJqKxAQAAAGB7NDYAAAAAbM/jxubLL7+UAQMGSFxcnISEhMiaNWt0X9c0TZ5++mlp2bKlRERESHJyshw8eNBX+SLIffPNN9QfTEP9wWzUIMxE/cHqPN48oLi4WDp16iSjR4+Wu+++W/n67NmzZe7cubJkyRJp27atTJs2Tfr16yd79+6V+vXr+yTpmjJaDF/bPPPMM27Nc2dDgY0bNyoxXy5m88TZs2dtX3+B4MuF+40aNVJiRovIXZ09e1aJXbhwwSc5mSWY6m/o0KFuxYyuBa5PThdhswBfCaYatKqysjKzUzAN9afKzs5WYtdee60SCw317kNS8+bNU2IdO3ZUYr179/bq/LWNx43NbbfdJrfddpvh1zRNk1dffVWeeuopGThwoIiIvP322xITEyNr1qyR4cOH1yxbBL1bb71VBg8ebPg16g/+Rv3BbNQgzET9wep8usbm0KFDkpeXJ8nJyc6Yw+GQxMREyczMNHxNaWmpFBUV6Q7AG97Unwg1CN+g/mA2/gbDTFwDYQU+bWwq9+SOiYnRxWNiYgz36xYRSU9PF4fD4Txat27ty5QQRLypPxFqEL5B/cFs/A2GmbgGwgpM3xVt6tSpUlhY6DxycnLMTglBhhqEmag/mIn6g9moQfiSx2tsqhIbGysiIvn5+dKyZUtnPD8/Xzp37mz4mvDwcAkPD/dlGtXq2bOnV69zd0G+VRnl787mAXbhTf2JmFODdmK0QNSdzQNatWrlj3Qsq7bVX1pamhKrqKhQYkYLYt2pD/ieXf4GW9XatWuV2N69e5XY/fffr8QWL16sG//888++Sss2ats10MiVV16pxIz+1rmzUcAHH3ygxNLT05VYvXr1lNgDDzygxPLz83XjkydPKnOuueaaavOyO5/esWnbtq3ExsbK+vXrnbGioiLZunWrJCUl+fKtAAX1BzNRfzAbNQgzUX+wAo/v2Jw5c0Z++OEH5/jQoUOye/duiY6Olvj4eElLS5Pnn39e2rVr59zqLy4uTgYNGuTLvBGkzpw5Iz/99JNzTP0hkKg/mI0ahJmoP1idx43Njh07dHtlT5o0SURERo4cKYsXL5ZHH31UiouLZezYsVJQUCDdu3eXTz75pNbuX47Ays7Olj/+8Y/OMfWHQKL+YDZqEGai/mB1IZrFPgxdVFQkDofDr+/h7bds1kMp/cno4ZvuPMB006ZNSiwQD4cqLCyUqKgov75HIGrQqi6//HIlZvTUaHf+G/rLX/6ixDIyMrxLzCKCqf6M1tMY/d6NrovDhg1TYitXrvRNYkHO3zVolfqzipEjRyqxt956S4m98MILuvGiRYuUOUeOHPFdYiYJpmugkTFjxiixv//972699qWXXtKNp02bpswx+neU0RqeRx99VIldccUVbuXhyuh7surfanfqz/Rd0QAAAACgpmhsAAAAANgejQ0AAAAA26OxAQAAAGB7Pn1ApxXZ/aGa/rZ582Yl5s7mAUZzjGJGmwzAuow2D3DXhQsXdOPvvvuupunAREYbBfCATtQWzZs3V2L/vdtXpZSUFLfO9/jjj+vGeXl5ypx58+a5mR2s6qGHHvL6tRs2bNCNp0yZosxJTExUYkbPAHI9l4jotuEWEenbt6+nKdYK3LEBAAAAYHs0NgAAAABsj8YGAAAAgO3R2AAAAACwvVq/eYAv1cbF8UabK0yfPt2rc9XGnw+MnyxvpKysTDfesWOHP9JBgBj93o02CjCa527NeKt169ZK7IYbbqj2dWlpaUrMaGGu6+YHRt/PnDlzlNgjjzxSbQ7wP6M6jYuL041Hjx6tzPH2bx+CR7Nmzbx+bUZGhm4cExOjzDl58qQS+8c//qHEpk6dqsQ6deqkGxttHlBcXKzEcnJy1GRtjDs2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtlfrNw9gcbznnn32Wd2YBZXB4+abb1ZiRk+RN4rNnj3bLznBHEa/44qKCiVmtFDb6LXeevjhh5XYkCFDlFi3bt2qzcsof3e+T6NzGW1EwOYBgRcbG6vEnn76aSX2wAMPBCId1HIHDhxQYkY1aMR1s4AlS5Yoc1544QUltn//frfOP27cuGrnnDlzRont27fPrfPbBXdsAAAAANgejQ0AAAAA26OxAQAAAGB7NDYAAAAAbK/Wbx5gxGjBv9HGAEBtNmLECCU2ePBgt15bUFCgxD799NOapgQLGT58uBJbtmyZEgsJCXEr5o6hQ4cqsZdeesmt87tuAmA0x2gTAHfmuXsuBN5rr72mxO666y6fnX/9+vVKrE+fPj47P+zlnnvuUWJ33323EuvQoYMSW758uW787bffKnOMFve7Ky4urto5rhsYiIjceuutSiwjI8PrPMzGlRkAAACA7dHYAAAAALA9GhsAAAAAtheUa2w2b96sxNxZY9OzZ08/ZAOYw+jBq+3atXPrtStWrFBi27Ztq3FOsI7MzEwltmXLFiV24403KrGHHnpIia1cudKrPNx9KKjrvKysLLfOb5S/Ow/onDNnjlvnh+9MnDhRiQ0YMMCrc3333XdK7I477lBi1157rRJjjU3wys/PV2ILFiwIeB6tWrVSYkbrelyVlpYqsRMnTvgkJ6vgjg0AAAAA26OxAQAAAGB7NDYAAAAAbI/GBgAAAIDtBeXmAUYP6DRaSO3KaIOBjRs3KrHevXt7kxbgV08++aRufNlll7n1utOnTyuxmTNn+iQnWNfRo0eVWG5urhIzenilOwvyjRbfT548WYm5PnjzYu/56quv6sZGm1mkpaW5dS7XzQKOHTumzHn//feVGPzL6AHCdeuq/4y5cOGCElu7dq1unJqaqswxWkS9ZMkST1IEAsLo+rxv3z7d2GgzoOPHjyuxjz76yHeJWQB3bAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPzQP+z7PPPqsbu7OZgIjxhgJGMaP3tIJnnnlGibn7vbtzLpijTZs2Suypp57SjevUqePWuYzqIScnx6u8YG+vvPKKEhsyZIgSc90oQERdkG+0kN9oowB3zmV0PqM5Rucyek/XjQ2MNgrIyspSYrCGf//730ps2LBhuvEll1yizHnooYeUWGJiou8SA3ykQYMGSiwqKqra173zzjv+SMdSuGMDAAAAwPZobAAAAADYHo0NAAAAANvzqLFJT0+Xrl27SmRkpLRo0UIGDRok+/fv180pKSmRlJQUadq0qTRq1EgGDx4s+fn5Pk0awatXr17UH0wzZ84croEwFddAmIlrIKwuRDNaOXkR/fv3l+HDh0vXrl3lwoUL8sQTT8iePXtk79690rBhQxERGT9+vHz00UeyePFicTgckpqaKqGhofLNN9+49R5FRUXicDi8+258aOPGjUrMaFMAd7luHrB582ZljruL793ZsKBnz55uvc5bRk/rDoT58+dLjx49/FZ/ItapwZowWvD/5JNP6sbubh5w6aWXKjGjpx4Hgz59+sh9990XFNdAd91www1KbPny5UqsdevWurHRnx6j64q384zmGP0Obr75ZiVmZf6+Blq5/kaNGqXE3nzzTSW2Z88eJXbkyBHd2GiDlauvvtrr3Pbu3asbDxgwQJnz888/e31+q+AaaA3XX3+9Etu2bZtuXFZWpswZOnSoEvvwww99l5ifFRYWVrtJgke7on3yySe68eLFi6VFixayc+dO6dGjhxQWFsqbb74pS5culVtuuUVERDIyMuSqq66SrKwswz+ApaWlUlpa6hwXFRV5khKCzJ/+9CdnUfui/kSoQbjv/fff111UuQYi0Hx9DaT+4AmugbC6Gq2xKSwsFBGR6OhoERHZuXOnnD9/XpKTk51zOnToIPHx8ZKZmWl4jvT0dHE4HM7D9f/qARfji/oToQbhPa6BMBP1B7NRg7AarxubiooKSUtLk5tuukk6duwoIiJ5eXkSFhYmjRs31s2NiYmRvLw8w/NMnTpVCgsLnQfPx4A7fFV/ItQgvMM1EGai/mA2ahBW5PUDOlNSUmTPnj3y9ddf1yiB8PBwCQ8Pr9E5/KF3795KrCYPs3Rd32K03sXbB2P6mut6INeHl1qBr+pPxLo16K6mTZsqMdf1NCLuram54447lFiwrqepTm2/BrrL6EGV99xzjxJr1aqVbmz0MMQbb7xRiRk9VHPEiBFKzJ01NrXpoZrBWH/fffedEvvvjzBVqvxHdnUxd5w/f16Jua6nEVEfVFsb1tNUJxhr0E4OHTqkxOy0nsZbXt2xSU1NlXXr1snGjRt1f6xiY2OlrKxMCgoKdPPz8/MlNja2RokClag/mI0ahJmoP5iNGoRVedTYaJomqampsnr1atmwYYO0bdtW9/WEhASpV6+erF+/3hnbv3+/HDlyRJKSknyTMYLalClTqD+YhmsgzMY1EGbiGgir8+ijaCkpKbJ06VJZu3atREZGOj8v6XA4JCIiQhwOh4wZM0YmTZok0dHREhUVJRMnTpSkpKSL7kgFeOK9996j/mCayZMny8qVK6lBmIZrIMzENRBW51Fjs2DBAhFR14dkZGQ495d/5ZVXJDQ0VAYPHiylpaXSr18/mT9/vk+SBQoLC6k/mKbymRnUIMzCNRBm4hoIq/PoAZ2BUBsezFSTTQa85brg30hNHgpqFe48nKmm7FaD3bp1U2JVbW9dKTc3V4kZPdj1p59+8i6xWoj6g9n8XYN2q7+BAwcqsXbt2imxvn376sZ9+vRR5sycOVOJGW1YsGLFCk9SrFW4BlqDUa0+8sgjuvHzzz+vzJkxY4bfcgoEd+qvRs+xAQAAAAAroLEBAAAAYHs0NgAAAABsj8YGAAAAgO2xeQBshYWLqkaNGimxDz74QIldffXVunH//v2VObt37/ZZXrUR9QezsXkAzMQ10BpWrVqlxC6//HLduHPnzgHKJnDYPAAAAABAUKCxAQAAAGB7NDYAAAAAbI/GBgAAAIDt1TU7AQA1c+bMGSV2yy23mJAJAADwpcsuu0yJlZWVKbE6deroxvXq1VPmnD9/3neJWRR3bAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPzQMAAAAAC4qNjVVid911lxL7z3/+oxsbbTqwf/9+3yVmUdyxAQAAAGB7NDYAAAAAbI/GBgAAAIDtscYGAAAAsKAtW7Yosfr16yux5557TjcOhvU0RrhjAwAAAMD2aGwAAAAA2B6NDQAAAADbo7EBAAAAYHtsHgAAAADY2LRp08xOwRK4YwMAAADA9mhsAAAAANgejQ0AAAAA27NcY6NpmtkpwMICUR/UIC6G+oPZ/F0f1B+qwjUQZnKnNizX2Jw+fdrsFGBhgagPahAXQ/3BbP6uD+oPVeEaCDO5UxshmsVa44qKCsnNzZXIyEg5ffq0tG7dWnJyciQqKsrs1DxSVFRk29xFrJe/pmly+vRpiYuLk9BQ//bjlTWoaZrEx8db5mfgKav9Dj1lpfypP89Z6ffnDavlH6ga5G+wNVgtf66BnrPa79BTVsrfk/qz3HbPoaGh0qpVKxERCQkJERGRqKgo03+o3rJz7iLWyt/hcATkfSprsKioSESs9TPwBvn7BvXnHfL3nUDUIH+DrcVK+XMN9A75+4a79We5j6IBAAAAgKdobAAAAADYnqUbm/DwcJk+fbqEh4ebnYrH7Jy7iP3z9wW7/wzI397s/v2Tv/3Z+Wdg59xF7J+/L9j9Z0D+5rDc5gEAAAAA4ClL37EBAAAAAHfQ2AAAAACwPRobAAAAALZHYwMAAADA9mhsAAAAANieZRub119/Xdq0aSP169eXxMRE2bZtm9kpGfryyy9lwIABEhcXJyEhIbJmzRrd1zVNk6efflpatmwpERERkpycLAcPHjQnWRfp6enStWtXiYyMlBYtWsigQYNk//79ujklJSWSkpIiTZs2lUaNGsngwYMlPz/fpIwDixr0P2rw4qg//6P+Lo768z/qr2rUoP/Vxhq0ZGOzfPlymTRpkkyfPl127dolnTp1kn79+smJEyfMTk1RXFwsnTp1ktdff93w67Nnz5a5c+fKwoULZevWrdKwYUPp16+flJSUBDhT1ebNmyUlJUWysrLk888/l/Pnz0vfvn2luLjYOefhhx+WDz/8UFasWCGbN2+W3Nxcufvuu03MOjCowcCgBo1Rf4FB/Rmj/gKD+rs4ajAwamUNahbUrVs3LSUlxTkuLy/X4uLitPT0dBOzqp6IaKtXr3aOKyoqtNjYWO3FF190xgoKCrTw8HDt3XffNSHDqp04cUITEW3z5s2apv2Wa7169bQVK1Y453z//feaiGiZmZlmpRkQ1KA5qMHfUH/moP5+Q/2Zg/r7HTVojtpQg5a7Y1NWViY7d+6U5ORkZyw0NFSSk5MlMzPTxMw8d+jQIcnLy9N9Lw6HQxITEy35vRQWFoqISHR0tIiI7Ny5U86fP6/Lv0OHDhIfH2/J/H2FGjQPNUj9mYn6o/7MRP39hho0T22oQcs1NidPnpTy8nKJiYnRxWNiYiQvL8+krLxTma8dvpeKigpJS0uTm266STp27Cgiv+UfFhYmjRs31s21Yv6+RA2agxr8DfVnDurvN9SfOai/31GD5qgtNVjX7ARgDSkpKbJnzx75+uuvzU4FQYoahJmoP5iJ+oPZaksNWu6OTbNmzaROnTrKjgv5+fkSGxtrUlbeqczX6t9LamqqrFu3TjZu3CitWrVyxmNjY6WsrEwKCgp0862Wv69Rg4FHDf6O+gs86u931F/gUX961GDg1aYatFxjExYWJgkJCbJ+/XpnrKKiQtavXy9JSUkmZua5tm3bSmxsrO57KSoqkq1bt1rie9E0TVJTU2X16tWyYcMGadu2re7rCQkJUq9ePV3++/fvlyNHjlgif3+hBgOHGlRRf4FD/amov8Ch/oxRg4FTK2vQ1K0LLmLZsmVaeHi4tnjxYm3v3r3a2LFjtcaNG2t5eXlmp6Y4ffq0lp2drWVnZ2sior388stadna2dvjwYU3TNO2FF17QGjdurK1du1b79ttvtYEDB2pt27bVzp07Z3LmmjZ+/HjN4XBomzZt0o4fP+48zp4965wzbtw4LT4+XtuwYYO2Y8cOLSkpSUtKSjIx68CgBgODGjRG/QUG9WeM+gsM6u/iqMHAqI01aMnGRtM0bd68eVp8fLwWFhamdevWTcvKyjI7JUMbN27UREQ5Ro4cqWnab1v9TZs2TYuJidHCw8O1Pn36aPv37zc36f9jlLeIaBkZGc45586d0yZMmKA1adJEa9CggXbXXXdpx48fNy/pAKIG/Y8avDjqz/+ov4uj/vyP+qsaNeh/tbEGQzRN03xz7wcAAAAAzGG5NTYAAAAA4CkaGwAAAAC2R2MDAAAAwPZobAAAAADYHo0NAAAAANujsQEAAABgezQ2AAAAAGyPxgYAAACA7dHYAAAAALA9GhsAAAAAtkdjAwAAAMD2/j/5l7OXFNVvCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1000x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx in range(0,10):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,mnist_trainset.data.shape[0])\n",
    "    plt.imshow(mnist_trainset.data[rand_ind,:,:],cmap='gray')\n",
    "    plt.title(mnist_list[int(mnist_trainset.targets[rand_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzRfY8QTBIX-"
   },
   "source": [
    "## Defining the model for MNIST\n",
    "\n",
    "We will now define the simple CNN described above, for use with MNIST. The input of the CNN is a set of (28,28,1) image tensors. We apply the following layers:\n",
    "\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "    - a ReLu activation function\n",
    "    \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "    - We then Flatten the data: reduce them to a vector in order to be able to apply a Fully-Connected layer to it\n",
    "    - Dense (fully connected) layer. Note, you will have to determine the input size, that is to say the number of elements after the last Max Pooling layer.\n",
    "\n",
    "__VERY IMPORTANT NOTE !!!__\n",
    "\n",
    "Pytorch carries out the softmax which we would expect at the end of our network automatically, so there is no need to add it. Nevertheless, you must understand that the output is a vector which is _not_ normalised to be a probability distributuion. This will be important later on.\n",
    "\n",
    "Now, we define the following hyper-parameters of the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3T9d8TYFBONz"
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "nb_classes = int(mnist_trainset.targets.max()+1)\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "# --- Size of the successive layers\n",
    "n_h_0 = 1 #greyscale input images\n",
    "n_h_1 = nb_filters\n",
    "n_h_2 = nb_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MOihxZ-W1-W"
   },
   "source": [
    "# 1. Model 1 : defining a CNN with the Sequential API of Pytorch for MNIST\n",
    "\n",
    "We are now going to create the CNN with Pytorch.\n",
    "\n",
    "The Sequential approach is quite similar to that of Tensorflow. To define a model, just write:\n",
    "\n",
    "```my_model = torch.nn.Sequential( first_layer, second_layer, ...)```\n",
    "\n",
    "Each layer must be a function imported from the Pytorch. You can use the following functions:\n",
    "\n",
    "- ```torch.nn.Conv2d()```\n",
    "- ```torch.nn.ReLU()```\n",
    "- ```torch.nn.MaxPool2d()```\n",
    "- ```torch.nn.Flatten()```\n",
    "- ```torch.nn.Linear()```\n",
    "\n",
    "Look at the documentation online to find the correct parameters. For example:\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npx-4C1SW1-X"
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "convLayer1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=\"same\")\n",
    "reLULayer1 = torch.nn.reLU(convLayer1)\n",
    "convLayer2 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=\"same\")\n",
    "reLULayer2 = torch.nn.reLU(convLayer1)\n",
    "mnist_model = torch.nn.Sequential()\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FJS2SGeEwHF"
   },
   "source": [
    "### Define loss function and optimiser\n",
    "\n",
    "Pytorch provides an easy way to define the loss criterion to optimise. The syntax is (considering that the Adam optimiser is used):\n",
    "\n",
    "- ```criterion = torch.nn.BCELoss()``` or ```criterion = torch.nn.CrossEntropyLoss()```, etc., depending on your problem.\n",
    "- ```optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)```\n",
    "\n",
    "Fill in the following code, choosing the correct criterion to optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK1pxqFCE090"
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE \n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42zy2XwsNfTQ"
   },
   "source": [
    "### CNN prediction conversion\n",
    "\n",
    "We recall here that the output of the classification CNN in Pytorch is a vector which is __NOT__ normalised to be a probability distribution. Therefore, for the purposes of finding the prediction of the CNN, we create a function which first converts an input vector to a probability distribution, and then determines the most likely class for each vector. The output should be, for each vector, an integer between 0 and (number of classes) $-1$.\n",
    "\n",
    "The inputs to this function will be Pytorch tensors, so you can use the following Pytorch functions on them :\n",
    "\n",
    "- ```torch.nn.Softmax()```\n",
    "- ```torch.argmax()```\n",
    "\n",
    "Create this function now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqCPink-N1LB"
   },
   "outputs": [],
   "source": [
    "def vector_to_class(x):\n",
    "  # BEGIN STUDENT CODE\n",
    "  y = ...\n",
    "  # END STUDENT CODE\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYzRpoTgGhpG"
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Now, define a function which calcualtes the accuracy of the output of the neural network, with respect to the input labels. We consider that the input is a vector of class numbers (the final prediction of the CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4V5LKDhGsQT"
   },
   "outputs": [],
   "source": [
    "def cnn_accuracy(predict,labels):\n",
    "  # BEGIN STUDENT CODE\n",
    "  accuracy = ...\n",
    "  # END STUDENT CODE\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljHi0tfiW1-h"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Now, we carry out the actual training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVe9ZWAdW1-h"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "  train_loss=0.0\n",
    "\n",
    "  for batch_idx, (imgs, labels) in enumerate(mnist_train_loader):\n",
    "\n",
    "    # set the gradients back to 0\n",
    "    optimizer.zero_grad()\n",
    "    predict=... # FILL IN STUDENT\n",
    "    # apply loss function\n",
    "    loss=criterion(predict,labels)\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss=loss.item()\n",
    "  print('Epoch:{} Train Loss:{:.4f}'.format(epoch,train_loss/imgs.shape[0]))\n",
    "  # calculate accuracy\n",
    "\n",
    "  print('Accuracy:{:.4f}'.format(cnn_accuracy(vector_to_class(predict),labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOsF40hkEqx1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy on the training set and the test set\n",
    "\n",
    "# BEGIN FILL IN STUDENT (use X_train, Y_train, X_test, Y_test)\n",
    "predict_train = ...\n",
    "predict_test = ...\n",
    "\n",
    "\n",
    "train_accuracy = cnn_accuracy(...,...)\n",
    "test_accuracy = cnn_accuracy(...,...)\n",
    "# END FILL IN STUDENT\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRxCNvMO5Yzm"
   },
   "outputs": [],
   "source": [
    "print(\"Visual results : \")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx in range(0,10):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,X_test.shape[0])\n",
    "    test_img = torch.unsqueeze(X_test[rand_ind,:,:,:],axis=1)\n",
    "    predicted_class = vector_to_class(mnist_model(test_img))\n",
    "    plt.imshow(test_img.squeeze(),cmap='gray')\n",
    "    plt.title(mnist_list[int(predicted_class)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5D5BY20W1-m"
   },
   "source": [
    "## Model 2: create a CNN on CIFAR10\n",
    "\n",
    "We are now going to train the same network architecture on a more difficult dataset : CIFAR10\n",
    "\n",
    "First, we import the CIFAR10 data and carry out some pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k15abDeRW1-m"
   },
   "outputs": [],
   "source": [
    "# convert input to Pytorch tensors\n",
    "input_transform=transforms.Compose( [transforms.ToTensor()])\n",
    "# extract mnist data\n",
    "cifar_trainset = datasets.CIFAR10(root='./data',train=True,download=True,transform=input_transform)\n",
    "print(cifar_trainset)\n",
    "\n",
    "#create data loader with smaller dataset size\n",
    "max_cifar_size = 2000\n",
    "cifar_trainset_reduced = torch.utils.data.random_split(cifar_trainset, [max_cifar_size, len(cifar_trainset)-max_cifar_size])[0] \n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset_reduced, batch_size=256, shuffle=True)\n",
    "\n",
    "# download test dataset\n",
    "cifar_testset = datasets.CIFAR10(root='./data',train=False,download=True,transform=input_transform)\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=256, shuffle=True)\n",
    "\n",
    "# extract the actual data and labels\n",
    "X_train = cifar_trainset.data[0:max_cifar_size]/255.0\n",
    "Y_train = cifar_trainset.targets[0:max_cifar_size]\n",
    "X_test = cifar_testset.data/255.0\n",
    "Y_test = cifar_testset.targets\n",
    "\n",
    "nb_channels = X_train.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Al5b-S37T4A"
   },
   "outputs": [],
   "source": [
    "# the CIFAR10 categories\n",
    "cifar_10_list = [ 'airplane', 'automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "#modify the number of input channels \n",
    "n_h_0 = nb_channels\n",
    "# add more epochs\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpM3zdF_W1-x"
   },
   "source": [
    "### Display some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeFaoz2AW1-y"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx in range(0,10):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,X_test.shape[0])\n",
    "    plt.imshow(X_test[rand_ind,:,:,:])\n",
    "    plt.title(cifar_10_list[int(Y_test[rand_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrGI_L3OW1-3"
   },
   "source": [
    "## Define the architecture again, for CIFAR10\n",
    "\n",
    "In this case, we are going to add a layer because the data is more complex. Therefore, we use the following architecture :\n",
    "\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
    "    - Dense (fully connected) layer. Note, you will have to determine the input size, that is to say the number of elements after the last Max Pooling layer.\n",
    "    - ReLU activation function\n",
    "    - Dense (fully connected) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEs-EnXLW1-4"
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "\n",
    "# --- Size of the successive layers\n",
    "n_h_0 = nb_channels\n",
    "n_h_1 = nb_filters\n",
    "n_h_2 = nb_filters\n",
    "n_h_3 = nb_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0ACZkTFW1-7"
   },
   "source": [
    "Now, modify the previous code (you can copy/paste/modify the necessary parts) to define the model for CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmbX6oq0W1-7"
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "\n",
    "cifar_model = torch.nn.Sequential(...)\n",
    "\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUmQKIEe8_QU"
   },
   "source": [
    "Now, carry out training on the CIFAR10 dataset (use the previous code as an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyEwLnLR9gv0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# BEGIN STUDENT CODE\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afxhSlTZW1_A"
   },
   "source": [
    " What do you think about the results (better or worse than MNIST) ? Why do you think this is ? How could you improve the results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIvS5eHEW1_A"
   },
   "source": [
    "## Visualising the convolutional weights\n",
    "\n",
    "You can explore the network parameters easily with Pytroch. Indeed, your model ```cifar_model``` is simply a list of layers, thus you can access the first layer with :\n",
    "- ```cifar_model[0]```\n",
    "\n",
    "If you want to find out the contents of this layer, use :\n",
    "\n",
    "```dir(cifar_model[0])```\n",
    "\n",
    "In particular, the convolutional weights are contained in the ```weights``` sub-structure (a multi-dimensional array). Note that this weight's size is : \n",
    "\n",
    "- $[n_{filters}, y_{size}, x_{size},n_{channels}]$\n",
    "\n",
    "Now, display all (32) trained filters of the first convolutional layer, taking only the first channel of each filter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BahI3aUN-3fG"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for num in range(0,32):\n",
    "    plt.subplot(8, 4, num+1)\n",
    "    # --- START CODE HERE\n",
    "    plt.imshow( (...).detach().numpy(),cmap='gray')\n",
    "    # --- END CODE HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX6yNSiKBXDH"
   },
   "source": [
    "What do you think ? Pretty incomprehensible no ? Do not spend too long trying to interpret these filters, that way madness lies. Indeed, they say that you can find some researchers locked in their offices staring at convolutional filters, trying to find some semblance of meaning ...\n",
    "\n",
    "So, it seems that understanding a CNN by just looking at the filters is an exercise in futility. How can we do better ? Well, take a look at part 2 of the lab !"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
