{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f822629b",
   "metadata": {},
   "source": [
    "# <center> IMA205 - LAB3 Supervised Learning\n",
    "### <center> Antoine Andurao ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44feb7ca",
   "metadata": {},
   "source": [
    "# OLS #\n",
    "\n",
    "We assume that we are under the fixed design model, i.e. that :\n",
    "$$\n",
    "Y = X\\beta + \\epsilon \\\\\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Let's compute $\\mathbb{E}[\\tilde\\beta]$.\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}[\\tilde\\beta] = \\mathbb{E}[CY] = CX\\beta + C\\mathbb{E}[\\epsilon] = HX\\beta + DX\\beta\n",
    "$$\n",
    "\n",
    "Since $\\tilde\\beta$ is unbiased, we have $DX\\beta = 0$, $\\forall\\beta$\n",
    "\n",
    "Thus, $DX=0$\n",
    "\n",
    "Let's compute $\\text{Var}(\\tilde\\beta)$ :\n",
    "\n",
    "$$ \n",
    "\\text{Var}(\\tilde\\beta) = \\text{Var}(CY) = \\text{Var}(CX\\beta + C\\epsilon) = \\text{Var}(\\epsilon)CC^T = \\sigma^2(HH^T + HD^T + DH^T + DD^T) = \\sigma^2((X^TX)^{-1} + DD^T) = \\text{Var}(\\beta^*) + \\sigma^2DD^T\n",
    "$$\n",
    "$$\n",
    "\\sigma^2DD^T = \\sigma^2||D||_2^2 > 0\n",
    "$$\n",
    "\n",
    "Hence $ \\text{Var}(\\tilde\\beta) > \\text{Var}(\\beta^*) $  if $D\\neq0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8cb5f",
   "metadata": {},
   "source": [
    "# Ridge Regression #\n",
    "\n",
    "Under the fixed design model, we have that $\\beta^*_{ridge} = (\\lambda I_d + X^TX)^{-1}X^TY$.\n",
    "\n",
    "**Bias**\n",
    "\n",
    "$$ \n",
    "\\mathbb{E}[\\beta^*_{ridge}] = \\mathbb{E}[(\\lambda I_d + X^TX)^{-1}X^Ty] = \\mathbb{E}[(\\lambda I_d + X^TX)^{-1}X^TX\\theta] + \\mathbb{E}[(\\lambda I_d + X^TX)^{-1}X^T\\epsilon] = (\\lambda I_d + X^TX)^{-1}X^TX\\theta = (\\lambda I_d + X^TX)^{-1}(\\lambda I_d + X^TX)\\theta - \\lambda I_d(\\lambda I_d + X^TX)^{-1}\\theta = \\theta - \\lambda I_d(\\lambda I_d + X^TX)^{-1}\\theta\n",
    "$$\n",
    "\n",
    "The ridge estimator has a bias of $- \\lambda I_d(\\lambda I_d + X^TX)^{-1}\\theta$.\n",
    "\n",
    "**SVD Decomposition**\n",
    "\n",
    "We assume that $X = UDV^T$ :\n",
    "\n",
    "$$\n",
    "\\beta^*_{ridge} = (\\lambda I_d + X^TX)^{-1}X^T = (\\lambda VV^T + VD^2V^T)^{-1}VDU^T = (V(D^2 + \\lambda)V^T)^{-1}VDU^T = V(D^2 + \\lambda)^{-1}V^TVDU^T = V(D^2 + \\lambda)^{-1}DU^T\n",
    "$$\n",
    "\n",
    "**Variance**\n",
    "\n",
    "Let's compute $\\text{Var}(\\beta^*_{ridge})$ :\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\beta^*_{ridge}) = \\mathbb{E}[(\\beta^*_{ridge} - \\mathbb{E}[\\beta^*_{ridge}])(\\beta^*_{ridge} - \\mathbb{E}[\\beta^*_{ridge}])^T] = \\mathbb{E}[(V(D^2 + \\lambda)^{-1}DU^T\\epsilon)(V(D^2 + \\lambda)^{-1}DU^T\\epsilon)^T] = \\mathbb{E}[(V(D^2 + \\lambda)^{-1}DU^T\\epsilon\\epsilon^TUD(D^2 + \\lambda)^{-1}V^T] = \\sigma^2V(D^2 + \\lambda)^{-1}D^2(D^2 + \\lambda)^{-1}V^T\n",
    "$$\n",
    "\n",
    "Writing $D = \\text{diag}(d_i)$, we have $ \\text{Var}(\\beta^*_{ridge}) = \\sigma^2V\\text{diag}(\\frac{d_i^2}{(d_i^2 + \\lambda)^2})V^T$\n",
    "\n",
    "Thus : $ \\text{Var}(\\beta^*_{ridge}) -  \\text{Var}(\\beta^*_{OLS}) = \\sigma^2V\\text{diag}(\\frac{d_i^2}{(d_i^2 + \\lambda)^2} - \\frac{1}{d_i^2})V^T = \\sigma^2V\\text{diag}(\\frac{1}{d_i^2}( \\frac{1}{(1+ \\frac{\\lambda}{d_i^2})^2} - 1))V^T$.\n",
    "\n",
    "$\\forall i$, $(\\frac{1}{d_i^2}( \\frac{1}{(1+ \\frac{\\lambda}{d_i^2})^2} - 1) \\leq 0 $ as long as $\\lambda \\geq 0$.\n",
    "\n",
    "If $\\lambda \\geq 0$, we have that $\\text{Var}(\\beta^*_{ridge}) \\leq  \\text{Var}(\\beta^*_{OLS})$.\n",
    "\n",
    "**Influence of parameter $\\lambda$**\n",
    "\n",
    "For the variance, we have :\n",
    "\n",
    "$$\n",
    "\\lim_{\\lambda \\to +\\infty} \\text{Var}(\\beta^*_{ridge}) = 0 \\\\\n",
    "\\lim_{\\lambda \\to 0} \\text{Var}(\\beta^*_{ridge}) = \\text{Var}(\\beta^*_{OLS}) = \\sigma^2(X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "For the expected value, we have :\n",
    "\n",
    "$\\mathbb{E}[\\beta^*_{ridge}] = \\mathbb{E}[(\\lambda I_d + X^TX)^{-1}X^TX\\theta] = V\\text{diag}(\\frac{d_i^2}{(d_i^2 + \\lambda)})V^T\\theta$\n",
    "\n",
    "Thus : \n",
    "\n",
    "$$\n",
    "\\lim_{\\lambda \\to +\\infty} \\mathbb{E}[\\beta^*_{ridge}] = 0 \\\\\n",
    "\\lim_{\\lambda \\to 0} \\mathbb{E}[\\beta^*_{ridge}] = \\mathbb{E}[\\beta^*_{OLS}] = \\theta\n",
    "$$\n",
    "\n",
    "**Relation with OLS**\n",
    "\n",
    "Let's assume that $X^TX = I_d$, and recall that $\\beta^*_{ridge} = (\\lambda I_d + X^TX)^{-1}X^TY = \\frac{1}{\\lambda + 1}X^TY$ and $\\beta^*_{OLS} = (X^TX)^{-1}X^TY = X^TY$\n",
    "\n",
    "Hence $\\beta^*_{ridge} = \\frac{\\beta^*_{OLS}}{\\lambda + 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba81f7",
   "metadata": {},
   "source": [
    "# Elastic Net #\n",
    "\n",
    "Let's assume that $X^TX = I_d$, and recall that $\\beta^*_{OLS} = (X^TX)^{-1}X^TY = X^TY$\n",
    "\n",
    "Let $f$ be our objective function : $f(\\beta)=||Y - X\\beta||^2 + \\lambda_2||\\beta||^2_2 + \\lambda_1||\\beta||_1$\n",
    "\n",
    "Note that : $ ||\\beta||_1 = \\sum_{i=1}^n |\\beta_i|$, hence $\\nabla_\\beta(\\lambda_1||\\beta||_1) = \\pm \\lambda_1$\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta f (\\beta) = -2X^T(Y - X\\beta) + 2\\lambda_2 \\beta \\pm \\lambda_1 \\\\\n",
    "\\nabla_\\beta f (\\beta) = 0 \\iff 2X^TY \\pm \\lambda_1 = 2(1+\\lambda_2)\\beta\n",
    "$$\n",
    "\n",
    "Hence : $\\beta^*_{ElNet} = \\frac{\\beta^*_{OLS} \\pm \\frac{\\lambda_1}{2}}{(1+\\lambda_2)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
